apiVersion: v1
kind: ConfigMap
metadata:
  name: chutes-cache-cleaner
data:
  cache_cleanup.py: |
    import os
    import time
    import shutil
    from pathlib import Path
    from huggingface_hub import scan_cache_dir

    def get_dir_size(path):
        total = 0
        with os.scandir(path) as it:
            for entry in it:
                if entry.is_file():
                    total += entry.stat().st_size
                elif entry.is_dir():
                    total += get_dir_size(entry.path)
        return total

    def clean_safetensors_dir(dir_path, max_age_days, max_size_gb):
        if not dir_path or not os.path.exists(dir_path):
            return
        dir_size = get_dir_size(dir_path)
        max_size_bytes = max_size_gb * 1024 * 1024 * 1024
        cutoff_time = time.time() - (max_age_days * 24 * 3600)
        print(f"Current CivitAI directory size: {dir_size / 1024**3:.2f}GB")
        for file in Path(dir_path).glob("*.safetensors"):
            last_accessed = os.path.getatime(file)
            if last_accessed < cutoff_time:
                file_size = os.path.getsize(file)
                print(f"Removing old safetensors: {file.name} ({file_size / 1024**3:.2f}GB, "
                      f"last accessed: {time.strftime('%Y-%m-%d', time.localtime(last_accessed))})")
                os.remove(file)
        dir_size = get_dir_size(dir_path)
        if dir_size > max_size_bytes:
            files = [(f, os.path.getsize(f)) for f in Path(dir_path).glob("*.safetensors")]
            files.sort(key=lambda x: x[1], reverse=True)
            for file_path, size in files:
                if os.path.exists(file_path):
                    print(f"Removing large safetensors: {file_path.name} ({size / 1024**3:.2f}GB)")
                    os.remove(file_path)
                    dir_size = get_dir_size(dir_path)
                    if dir_size <= max_size_bytes:
                        break

    def clean_old_cache(max_age_days=5, max_size_gb=100):
        # Clean HuggingFace cache
        cache_info = scan_cache_dir()
        max_size_bytes = max_size_gb * 1024 * 1024 * 1024
        cutoff_time = time.time() - (max_age_days * 24 * 3600)
        print(f"Current HF cache size: {cache_info.size_on_disk / 1024**3:.2f}GB")
        repos = sorted(cache_info.repos, key=lambda r: r.last_accessed)
        for repo in repos:
            if repo.last_accessed < cutoff_time:
                print(f"Removing old cache: {repo.repo_id} ({repo.size_on_disk / 1024**3:.2f}GB, "
                      f"last accessed: {time.strftime('%Y-%m-%d', time.localtime(repo.last_accessed))})")
                shutil.rmtree(repo.repo_path)
        cache_info = scan_cache_dir()
        if cache_info.size_on_disk > max_size_bytes:
            repos = sorted(cache_info.repos, key=lambda r: r.size_on_disk, reverse=True)
            for repo in repos:
                if repo.repo_path.exists():
                    print(f"Removing large cache: {repo.repo_id} ({repo.size_on_disk / 1024**3:.2f}GB)")
                    shutil.rmtree(repo.repo_path)
                    cache_info = scan_cache_dir()
                    if cache_info.size_on_disk <= max_size_bytes:
                        break
        civitai_dir = os.getenv("CIVITAI_HOME")
        if civitai_dir:
            clean_safetensors_dir(civitai_dir, max_age_days, max_size_gb)

    if __name__ == "__main__":
        try:
            clean_old_cache(
                max_age_days=int(os.getenv("CACHE_MAX_AGE_DAYS", "5")),
                max_size_gb=int(os.getenv("CACHE_MAX_SIZE_GB", "500")),
            )
        except Exception as exc:
            print(f"Error cleaning up cache: {exc}")
